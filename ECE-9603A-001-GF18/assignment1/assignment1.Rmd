---
title: "ECE-9603A Assignment 1: Forecasting"
author: "Joe Abley -- jabley@uwo.ca"
date: 2018-10-10
output:
  pdf_document:
    toc: true
---

# Abstract

This paper is submitted for Assignment 1, ECE-9603A, Fall 2018, Western University Faculty of Engineering, Department of Electrical and Computer Engineering. It has been written in R Markdown\footnote{\url{https://rmarkdown.rstudio.com}}; the code used to produce the output included in this document is included with the document source\footnote{\url{https://github.com/ableyjoe/uwo-mesc/tree/master/ECE-9603A-001-GF18/assignment1}}.

The subject of this assignment is experimentation with different forecasting approaches and algorithms.

\newpage

```{r echo = FALSE, message = FALSE}
library(e1071)
library(rpart)
library(randomForest)
library(ggplot2)
library(scales)
library(gbm)
```

# Forecasting Problem

Given a dataset that describes various features of individual houses along with details of their sale, identify and test forecasting models that are able to predict the prices realised by the sale of houses based on an appropriate set of parameters.

# Available Data

We make use of a dataset that describes house sales in Iowa, published on and retrieved from Kaggle as part of a Kaggle\footnote{\url{https://www.kaggle.com/}} competition entitled "House Prices: Advanced Regression Techniques"\footnote{\url{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data}}. This data set was suggested in directions for this assignment.

## Importing Data

Two datasets are provided, a training set and a test set:

```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

However, the `test` set does not contain a `SalePrice` column: the objective of the competition is to populate one with predicted values. The `train` set does, however.

```{r}
summary(train$SalePrice)
summary(test$SalePrice)
```

In order to cross-validate the accuracy of the predictions used by different models it will be convenient to have a test set that includes a `SalePrice` column. We shall therefore discard the supplied test set and construct a replacement from the supplied train set. The train set will be reduced correspondingly in order to avoid contamination.

```{r}
data <- train

# start again to ensure the intersection between test and train is null
rm(test)
rm(train)

sample <- sample.int(n = nrow(data), size = floor(0.2 * nrow(data)))
train <- data[-sample,]
test <- data[sample,]
```

## Abridged Feature Engineering

```{r}
dim(train)
```

There are 1168 rows in this dataset and 81 columns. Of those columns one is a numeric id and one is the sale price; the other 79 are parameters that describe each house, some of which are numeric variables and some of which are categories.

From the description provided with the source data, the numeric variables are as follows:

Variable Name   | Description
--------------  | -----------
`LotFrontage`   | Linear feet of street connected to property
`LotArea`       | Lot size in square feet
`MasVnrArea`    | Masonry veneer area in square feet
`BsmtFinSF2`    | Type 2 finished square feet
`BsmtUnfSF`     | Unfinished square feet of basement area
`TotalBsmtSF`   | Total square feet of basement area
`1stFlrSF`      | First Floor square feet
`2ndFlrSF`      | Second floor square feet
`LowQualFinSF`  | Low quality finished square feet (all floors)
`GrLivArea`     | Above grade (ground) living area square feet
`BsmtFullBath`  | Basement full bathrooms
`BsmtHalfBath`  | Basement half bathrooms
`FullBath`      | Full bathrooms above grade
`HalfBath`      | Half baths above grade
`Bedroom`       | Bedrooms above grade (does NOT include basement bedrooms)
`Kitchen`       | Kitchens above grade
`TotRmsAbvGrd`  | Total rooms above grade (does not include bathrooms)
`Fireplaces`    | Number of fireplaces
`GarageCars`    | Size of garage in car capacity
`GarageArea`    | Size of garage in square feet
`WoodDeckSF`    | Wood deck area in square feet
`OpenPorchSF`   | Open porch area in square feet
`EnclosedPorch` | Enclosed porch area in square feet
`3SsnPorch`     | Three season porch area in square feet
`ScreenPorch`   | Screen porch area in square feet
`PoolArea`      | Pool area in square feet
`MiscVal`       | \$Value of miscellaneous feature

The stated purpose of this assignment is "to experiment with different models"" and its focus is "applying forecasting approaches and not on optimising models". In the spirit of that direction we will not complete a detailed feature analysis and instead will select a set of numeric samples that seem likely to be sufficiently representative to give some kind of correlation, based on general background knowledge gained buying and selling houses in places other than Iowa, because how different can people from Iowa be? A smaller set of features seems helpful.

We can construct a new variable `newBathrooms`, derived from the various other bathroom variables:

 * `newBathrooms` (Total number of bathrooms, full and half, all levels) = `BsmtFullBath` + `BsmtHalfBath` + `FullBath` + `HalfBath`

```{r}
train$newBathrooms = train$BsmtFullBath + train$BsmtHalfBath + train$FullBath + train$HalfBath
```

We can eliminate some anticipated redundancy by identifying variables that seem likely to be closely related, and arbitrarily choosing the one that seems most interesting.

 * `LotFrontage` and `LotArea` both relate to the size of the lot, which seems pertinent. Retain `LotArea`.
 * `BsmtFinSF2`, `BsmtUnfSF` and `TotalBsmtSF` all relate to the size of the basement. Retain `TotalBsmtSF`.
 * `1stFlrSF`, `2ndFlrSF`, `LowQualFinSF` and `GrLivArea` all relate to the size of the rest of the house. Retain `GrLivArea`.
 * `Bedroom`, `Kitchen` and `TotRmsAbvGrd` all relate to the number of rooms above the basement. Retain `TotRmsAbvGrd`.
 * `GarageCars` and `GarageArea` both relate to the size of the garage. Retain `GarageArea`.
 
 We can keep some variables as-is, because they seem harmless and potentially interesting:
 
  * `Fireplaces`
 
We arbitrarily declare all remaining variables to be uninteresting. We take care to retain `SalePrice` which is our outcome/response variable.

```{r}
interesting <- c("newBathrooms", "LotArea", "TotalBsmtSF", "GrLivArea", "TotRmsAbvGrd",
  "Fireplaces", "GarageArea", "SalePrice")
train <- train[, (names(train) %in% interesting)]
```

To avoid surprises, we check for variables that might have missing data. Fortunately we seem not to have any.

```{r}
which(colSums(is.na(train)) > 0)
```

Our cauterised training data set now looks like this:

```{r}
summary(train)
```

Finally, we transform and reduce our test data set in the same way, since keeping it the same seems less likely to be confusing.

```{r}
test$newBathrooms = test$BsmtFullBath + test$BsmtHalfBath + test$FullBath + test$HalfBath
test <- test[, (names(test) %in% interesting)]
```

\newpage

# Data Inspection

## SalePrice

The distribution of sale prices is not symmetrical; there are more houses sold at lower prices and a long tail of expensive houses as is shown in the following histogram.

```{r}
ggplot(data=train[!is.na(train$SalePrice),], aes(x = SalePrice)) +
  geom_histogram(binwidth = 10000) +
  scale_x_continuous(breaks = seq(0, 800000, by = 100000), labels = comma)
```

This distribution is observed to be skewed to the right, as can be confirmed numerically:

```{r}
skewness(train$SalePrice)
```

The positive result confirms a skew to the right. We transform `SalePrice` data by replacing it with `log(SalePrice + 1)`:

```{r}
train$SalePrice <- log(train$SalePrice + 1)
summary(train$SalePrice)
skewness(train$SalePrice)
```

The skew is now much closer to zero, as can be confirmed visually:

```{r}
ggplot(data=train, aes(x = SalePrice)) +
  geom_histogram(binwidth = 0.05) +
  scale_x_continuous(breaks = seq(0, 20, by = 1), labels = comma)
```

We apply the same logarithmic transform to the test set:

```{r}
test$SalePrice <- log(test$SalePrice + 1)
```

\newpage

## newBathrooms

The total number of bathrooms seems to correlate to `SalePrice`, although there are a small number of outliers that suggest that at some point you really don't get much value from adding more toilets.

```{r}
ggplot(data=train, aes(x=newBathrooms, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

The houses with six bathrooms don't seem to fit a linear relationship very well. Since they represent a tiny minority of the observations they will be eliminated as outliers.

```{r}
train <- train[train$newBathrooms < 6, ]

ggplot(data=train, aes(x=newBathrooms, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

\newpage

## LotArea

For many houses there seems to be a strong correlation between `LotArea` and `SalePrice`. As with the tentative toilet hypothesis, however, it seems possible that the size of the lot beyond a certain point just starts to seem more annoying to mow.

```{r}
ggplot(data=train, aes(x=LotArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We will try to transform the `LotArea` variable as `log(LotArea)` to see whether that provides a more convincing linear relationship.

```{r}
train$LotArea <- log(train$LotArea + 1)
summary(train$LotArea)

ggplot(data=train, aes(x=LotArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

This looks slightly more convincing, although it does not show strong correlation. We will apply the same logarithmic transform to the test set.

```{r}
test$LotArea <- log(test$LotArea + 1)
```

\newpage

## TotalBsmtSF

A positive correlation is observed between `TotalBsmtSF` and `SalePrice`, with just a single outlier that we might imagine corresponds to a basement that is over-large for an unsavoury reason.

```{r}
ggplot(data=train, aes(x=TotalBsmtSF, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks = seq(0, 20, by=1), labels = comma)
```

We remove the property with the lurking, sub-grade menace, and also scale the variable to bring it into the same order of magnitude as the other variables considered so far:

```{r}
train <- train[train$TotalBsmtSF < 4000, ]
train$TotalBsmtSF <- (train$TotalBsmtSF / 200)

ggplot(data=train, aes(x=TotalBsmtSF, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks = seq(0, 20, by=1), labels = comma)
```

We scale the corresponding variable in the test set in the same manner.

```{r}
test$TotalBsmtSF <- (test$TotalBsmtSF / 200)
```

\newpage

## GrLivArea

There is a strong linear correlation observed between `GrLivArea` and `SalePrice`. The properties with a living area over 4,000 square feet seem to be outliers, but we don't expect them to exert too much influence over the model so we'll pretend we didn't notice.

```{r}
ggplot(data=train, aes(x=GrLivArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We scale the values in the training set to bring them into the same order of magnitude as the other variables:

```{r}
train$GrLivArea = (train$GrLivArea / 200)

ggplot(data=train, aes(x=GrLivArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We apply the same scaling transformation to the test set:

```{r}
test$GrLivArea = (test$GrLivArea / 200)
```

\newpage

## TotRmsAbvGrd

There is a strong linear correlation observed between `TotRmsAbvGrd` and `SalePrice`.

```{r}
ggplot(data=train, aes(x=TotRmsAbvGrd, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

\newpage

## Fireplaces

We observe a plausible correlation between the number of fireplaces and the sale price, although properties with three fireplaces seem to be outliers. People in Iowa like to burn things, but not *that* much.

```{r}
ggplot(data=train, aes(x=Fireplaces, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We shall remove the pyromaniac palaces from the training set:

```{r}
train <- train[train$Fireplaces < 3, ]

ggplot(data=train, aes(x=Fireplaces, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

\newpage

## GarageArea

Properties with more garage space seem to command higher prices. The extremely large garages seem to have a lower impact on price, but there are not so many batcaves that we expect them to cause trouble.

```{r}
ggplot(data=train, aes(x=GarageArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We scale the values down into the same order of magnitude as the others:

```{r}
train$GarageArea = (train$GarageArea / 100)

ggplot(data=train, aes(x=GarageArea, y=SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, colour = "blue", aes(group = 1)) +
  scale_y_continuous(breaks= seq(0, 20, by=1), labels = comma)
```

We apply the same transform to the test set:

```{r}
test$GarageArea = (test$GarageArea / 100)
```

\newpage

# Selected Algorithms

## Multivariate Regression

Multivariate linear regression is a method of supervised regression, used to predict a numerical outcome from a set of observations. In this exercise we have identified seven features (`LotArea`, `TotalBsmtSF`, `GrLivArea`, `TotRmsAbvGrd`, `Fireplaces`, `GarageArea` and `newBathrooms`); we have eliminated a small number of outliers and applied a logarithmic transform to some variables with the result that each of those features is observed to have a (different) linear relationship with `SalePrice`. Consequently, we will build and test a multivariate regression model with no further transforms and assess its goodness of fit.

```{r}
modelLR <- lm(SalePrice ~ LotArea + TotalBsmtSF + GrLivArea +
  TotRmsAbvGrd + Fireplaces + GarageArea + newBathrooms, train)
summary(modelLR)
predictLR <- predict(modelLR, test)

summary(test$SalePrice)
summary(predictLR)

# rmse
RMSE <- function(x, y) {
  a <- sqrt(mean((x - y)^2))
  return(a)
}

RMSE(test$SalePrice, predictLR)
```

## Support Vector Regression

Support Vector Regression (SVR) is another method of supervised regression. SVR is an adaptation of Support Vector Machines for function estimation, and is built around analogous hyperparameters, of which we are principally concerned with the soft margin loss setting $\epsilon$, an acceptable error in the resulting regression model. We make no attempt to tune the default parameters in the model used here.

```{r}
modelSVR <- svm(SalePrice ~ LotArea + TotalBsmtSF + GrLivArea +
  TotRmsAbvGrd + Fireplaces + GarageArea + newBathrooms, train)
summary(modelSVR)
predictSVR <- predict(modelSVR, test)

summary(test$SalePrice)
summary(predictSVR)

RMSE(test$SalePrice, predictSVR)
```

## Regression Trees

Decision trees attempt to classify an observation in the form of a target variable based on a set of input variables. They take the form of a directed graph where each interior node corresponds to a decision made on the basis of an input variable. A regression tree is a decision tree whose target variable is continuously variable, as is the case here. The training algorithm used by the library shown below uses recursive partitioning with an exit condition based on the target observation in the training set; the decision tree can then be used with a test dataset to produce predictions.

```{r}
modelRT <- rpart(SalePrice ~ LotArea + TotalBsmtSF + GrLivArea +
  TotRmsAbvGrd + Fireplaces + GarageArea + newBathrooms, data = train, method = "anova")
summary(modelRT)
predictRT <- predict(modelRT, test)

summary(test$SalePrice)
summary(predictRT)

RMSE(test$SalePrice, predictRT)
```

## Random Forests

The Random Forest algorithm is a development of those used to build regression trees that are able to correct for overfitting to the training set by constructing a large number of decision trees during training and producing an average of individual trees.

Parameters for the for this random forest model were copied from similar descriptions of kernels found at Kaggle that used the same library\footnote{e.g. see \url{https://www.kaggle.com/myonin/prediction-of-house-prices-3-methods}}.

```{r}
modelRF <- randomForest(SalePrice ~ LotArea + TotalBsmtSF + GrLivArea +
  TotRmsAbvGrd + Fireplaces + GarageArea + newBathrooms, data = train, method = "anova",
  ntree = 300, replace = FALSE, nodesize = 1, importance = TRUE)
summary(modelRF)
predictRF <- predict(modelRF, test)

summary(test$SalePrice)
summary(predictRF)

RMSE(test$SalePrice, predictRF)
```

## Gradiant Boosting Regression

Gradiant boosting machines are yet more examples of algorithms based on an ensemble of individually-weaker decision trees, generalising their output by the optimisation of a differentiable loss function.

Parameters for the for this GBM model were copied from similar descriptions of kernels found at Kaggle that used the same library\footnote{e.g. see \url{https://www.kaggle.com/myonin/prediction-of-house-prices-3-methods}}.

```{r}
modelGBM <- gbm(SalePrice ~ LotArea + TotalBsmtSF + GrLivArea +
  TotRmsAbvGrd + Fireplaces + GarageArea + newBathrooms, data = train,
  distribution = "laplace", shrinkage = 0.05, interaction.depth = 5,
  bag.fraction = 0.66, n.minobsinnode = 1, cv.folds = 100, keep.data = FALSE,
  verbose = FALSE, n.trees = 300)
summary(modelGBM)
predictGBM <- predict(modelGBM, test, n.trees = 300)

summary(test$SalePrice)
summary(predictGBM)

RMSE(test$SalePrice, predictGBM)
```

\newpage

# Accuracy Comparison

The source dataset used in this assignment was split arbitrarily into a training set (`train`) and a validation set (`test`) of first-seen data. Each of the models used were trained using the former and cross-validated using the latter, in effect testing the model against data that was not used in estimating it. The accuracy of the model was quantified as the root mean squared error between target variables in the validation set and the corresponding predicted values generated by each of the models.

Regression Model          | RMSE
------------------------- | ---------
Random Forests            | 0.1678204
Gradiant Boosting Machine | 0.1692927
Multivariate Regression   | 0.1789892
Support Vector Regression | 0.1980624
Regression Trees          | 0.2230123

No attempt was made to tune most of the models, with the notable exception of the two that gave the lowest RMSE which were tuned using parameters used by a Kaggle competitor working on the same dataset (albeit a dataset that was likely transformed differently, and probably not reduced with such viciousness). It seems entirely plausible that other models would see lower RMSE than that observed if some effort was made to tune them.
